{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53aec12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://localhost/output-events already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2cde5bfb6023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m process_events(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hdfs://localhost/input-events/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hdfs://localhost/output-events/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-2cde5bfb6023>\u001b[0m in \u001b[0;36mprocess_events\u001b[0;34m(input_path, output_path, checkpoint_location)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mevents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_events_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mevents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdedup_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrigger_events_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-2cde5bfb6023>\u001b[0m in \u001b[0;36mtrigger_events_processing\u001b[0;34m(events_df, output_path, checkpoint_location)\u001b[0m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     return events_df.write.partitionBy('domain', 'event_type', 'year', 'month', 'day').parquet(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://localhost/output-events already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "EVENT_SCHEMA = st.StructType([\n",
    "    st.StructField(\"event_id\", st.StringType()),\n",
    "    st.StructField(\"timestamp\", st.TimestampType()),\n",
    "    st.StructField(\"domain\", st.StringType()),\n",
    "    st.StructField(\"event_type\", st.StringType()),\n",
    "    st.StructField(\"data\", st.StringType())\n",
    "])\n",
    "\n",
    "spark_session = SparkSession.builder.appName(\"Pismo Challenge\").getOrCreate()\n",
    "\n",
    "def read_events_data(input_path):\n",
    "    events_data = spark_session.read.json(\n",
    "        input_path,\n",
    "        schema=EVENT_SCHEMA\n",
    "    )\n",
    "    return events_data\n",
    "\n",
    "def dedup_events(events_df):\n",
    "    last_timestamp = Window.partitionBy(\"event_id\") \\\n",
    "    .orderBy(sf.col(\"timestamp\").desc())\n",
    "    \n",
    "    events_df = events_df.distinct() \\\n",
    "           .withColumn('distinct', sf.row_number().over(last_timestamp)) \\\n",
    "           .filter('distinct == 1') \\\n",
    "           .drop('distinct')\n",
    "    \n",
    "    return events_df\n",
    "\n",
    "def trigger_events_processing(events_df, output_path, checkpoint_location):\n",
    "    events_df = events_df.withColumn(\n",
    "        'year', sf.year(sf.col('timestamp'))\n",
    "    ).withColumn(\n",
    "        'month', sf.month(sf.col('timestamp'))\n",
    "    ).withColumn(\n",
    "        'day', sf.dayofmonth(sf.col('timestamp'))\n",
    "    )\n",
    "   \n",
    "    return events_df.write.partitionBy('domain', 'event_type', 'year', 'month', 'day').parquet(\n",
    "        path=output_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_events(input_path, output_path, checkpoint_location):\n",
    "    events_data = read_events_data(input_path)\n",
    "    events_data = dedup_events(events_data)\n",
    "    trigger_events_processing(events_data, output_path, checkpoint_location).awaitTermination()\n",
    "\n",
    "    \n",
    "process_events(\n",
    "    input_path='hdfs://localhost/input-events/', \n",
    "    output_path='hdfs://localhost/output-events/', \n",
    "    checkpoint_location='hdfs://localhost/checkpoint-events/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2ac27556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_id: string, timestamp: timestamp, domain: string, event_type: string, data: string]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_events_data('hdfs://localhost/input-events/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9d8fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-------------------+-----------+-------+-------------+----+-----+---+\n",
      "|event_id                            |timestamp          |data       |domain |event_type   |year|month|day|\n",
      "+------------------------------------+-------------------+-----------+-------+-------------+----+-----+---+\n",
      "|c498e23b-4eb5-4402-ab93-dc8480bc4f03|2011-04-08 05:45:55|{\"id\":8754}|account|status-detail|2011|4    |8  |\n",
      "+------------------------------------+-------------------+-----------+-------+-------------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_id = 'c498e23b-4eb5-4402-ab93-dc8480bc4f03'\n",
    "spark_session.read.parquet('hdfs://localhost/output-events').filter(f'event_id = \"{event_id}\"').show(100, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
